#!/bin/bash

# -----------------------------------------------
# Pre-Requisites
# -----------------------------------------------

# Exit immediately if a command exits with a non-zero status
set -e

# Prevent interactive prompts during package installation
export DEBIAN_FRONTEND=noninteractive

cd /home/ubuntu

sudo apt update -y
sudo apt-get -y upgrade --no-install-recommends

# Install tools required in the script ahead
sudo apt-get install -y --no-install-recommends wget curl gpg gnupg net-tools unzip
sudo apt-get install -y --no-install-recommends e2fsprogs coreutils python3 python3-pip cron
sudo apt-get install -y --no-install-recommends openjdk-8-jdk

# Install AWS CLI
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install

echo "✅ Pre-Requisites are met"

# -----------------------------------------------
# Install & Configure Kafka
# -----------------------------------------------

# Download & Extract kafka setup files
wget https://dlcdn.apache.org/kafka/3.8.0/kafka_2.13-3.8.0.tgz
tar -xzf kafka_2.13-3.8.0.tgz

# Move Kafka to /opt directory
sudo mv kafka_2.13-3.8.0 /opt/kafka
sudo chown -R ubuntu:ubuntu /opt/kafka

cd /opt/kafka/

# Download & Configure Zookeeper Service
aws s3 cp ${zookeeper_service_s3_uri} /etc/systemd/system/zookeeper.service

# Download & Configure Kafka Service
aws s3 cp ${kafka_service_s3_uri} /etc/systemd/system/kafka.service

sudo systemctl daemon-reload

# Enable & Start Zookeeper service
sudo systemctl enable zookeeper.service
sudo systemctl start zookeeper.service

# -----------------------------------------------
# Kafka Configuration Tips
# -----------------------------------------------
# Storage:
#   Total log size ≈ number of partitions × log.retention.bytes
#   Example:
#     log.retention.hours=3
#     log.retention.bytes=1027604480 (1GB)
#     segment.bytes=134217728 (128MB)
#     log.segment.delete.delay.ms=30000 (30 seconds)
#   44 partitions * 1027604480 bytes = 45214597120 bytes = 45.214597120 GB
#   So a disk of at least 50GB is required for 44 partitions
# -----------------------------------------------

# Enable & Start Kafka service
sudo systemctl enable kafka.service
sudo systemctl start kafka.service

echo "⏳ Waiting for Kafka to start..."
until nc -zv 127.0.0.1 9092; do sleep 2; done

# Loop through the topics using terraform map keys
%{ for topic_name, partitions in kafka_topics }
bin/kafka-topics.sh \
    --create \
    --topic "${topic_name}" \
    --partitions "${partitions}" \
    --replication-factor 1 \
    --bootstrap-server 127.0.0.1:9092
%{ endfor }

echo "✅ Kafka installation and configuration is complete"

# -----------------------------------------------
# Install & Configure Monitoring Agents & Scripts
# -----------------------------------------------

# Allow commands to fail without stopping the script as all steps below 
# are only for monitoring setup which is not critical to Kafka setup
set +e

# Download custom cloudwatch agent configuration to export essential infrastructure metrics from S3
aws s3 cp ${cloudwatch_config_s3_uri} /home/ubuntu/cwa_config.json

# Install the AWS CloudWatch Agent
wget https://s3.amazonaws.com/amazoncloudwatch-agent/ubuntu/amd64/latest/amazon-cloudwatch-agent.deb
sudo dpkg -i -E ./amazon-cloudwatch-agent.deb

# Import custom cloudwatch agent configuration
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/home/ubuntu/cwa_config.json

# Prepare python environment to execute the script
pip3 install statsd pandas jq

# Download custom python script to export Kafka consumer group metrics to cloudwatch from S3
aws s3 cp ${statsd_script_s3_uri} /home/ubuntu/metrics_statsd.py

cd /home/ubuntu

# Run the metrics script in background
nohup python3 /home/ubuntu/metrics_statsd.py > /home/ubuntu/metrics_statsd.logs 2>&1 &

echo "✅ Kafka setup is complete"